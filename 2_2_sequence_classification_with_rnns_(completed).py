# -*- coding: utf-8 -*-
"""2-2 Sequence Classification with RNNs (Completed).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fzYML3T-9xUNthozBkkawErBxlQEraKW

## Sequential MNIST with RNNs
In This section we are going to classify MNIST dataset sequentially using RNNs.
IMAGE

** 0. In the first cell, all required packages and functions/classes are imported.**

Please run this cell.
"""

# import what you need
import keras
import matplotlib.pyplot as plt #This package is for plotting
# %matplotlib inline  
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Input, SimpleRNN, LSTM, GRU
from keras.optimizers import SGD
from keras.initializers import RandomNormal
from keras.models import load_model

"""** 1. Preparing data: **

IMPORTANT NOTE: You may use the same code as Hands-on1 for loading and preparing data; however, you should not flatten input data.

In this cell, following steps should be taken: 
* Load Train and Test data (use mnist.load_data())
* normalize data by deviding by its max (use numpy max function)
* change representation of label data to one-hot (use keras.utils.to_categorical)
* print the shape of all data arrays.
"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train/np.max(x_train)
x_test = x_test/np.max(x_test)
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)
print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))
print('X_train:', x_train.shape)
print('y_train:', y_train.shape)
print('y_test:', x_test.shape)
print('y_test:', y_test.shape)

"""** 2. Define Model **
In this cell we are going to define the model.Take following steps in order:
* (2.1) Determine number of hidden units(nb_units) for the RNN cell, sequence length, and feature size. (Note: You start using 50 hidden neurons, and then increase/decrease it to find the optimum number. The sequence length should be equal to image height and the feature size should be equal to image width.
* (2.2) We are going to create a Sequential model. (As in Hands-On 1)
* (2.3) Add a SimpleRNN layer to the model. (giving nb_units as the argument and input shape to the cell is enough for this task).
* (2.4) We should map the output of RNN cell (with size of nb_units) to our 10 class using a Dense layer. 
* (2.5) Use categorical_crossentropy as your loss and adam as your optimizer. You may add your desired metrics (accuracy is suggested). Compile the model using model.compile.
* (2.6) Print out model structure using model.summary(). You can check all the trainable parameters of your model.
"""

# 2.1 Determine the following variables:
nb_units = 50
seq_length = 28
feature_size = 28
# 2.2 Define a Sequential model. 
model = Sequential()
# 2.3 Add a SimpleRNN layer (search the documenation for its parameters)
model.add(SimpleRNN(nb_units, input_shape=(seq_length, feature_size)))
# 2.4 Add a Dense layer (search the documenation for its parameters)
model.add(Dense(units=10, activation='softmax'))
# 2.5 Compile the model.
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# 2.6 Print out model.summary
model.summary()

"""** 3. Training **
Simply Train the model using model.fit (check its parameter in keras documentation).
"""

epochs = 3

history = model.fit(x_train, 
                    y_train, 
                    epochs=epochs, 
                    batch_size=128,
                    verbose=1,
                    validation_split = 0.2)

plt.figure(figsize=(5,3))
plt.plot(history.epoch,history.history['loss'])
plt.title('loss')

plt.figure(figsize=(5,3))
plt.plot(history.epoch,history.history['acc'])
plt.title('accuracy');

"""** 4. Evaluate the model on Test data. **"""

scores = model.evaluate(x_test, y_test, verbose=2)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

"""** 5. Use other Cells **

In the definition of the model (Section 2), replace the SimpleRNN with LSTM (and GRU) and repeat the section 3 and 4.
"""

# 2.1 Determine the following variables:
nb_units = 50
seq_length = 28
feature_size = 28
# 2.2 Define a Sequential model. 
model = Sequential()
# 2.3 Add a LSTM layer (search the documenation for its parameters)
model.add(LSTM(nb_units, input_shape=(seq_length, feature_size)))
# 2.3 Add a Dense layer (search the documenation for its parameters)
model.add(Dense(units=10, activation='softmax'))
# 2.4 Compile the model.
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# 2.5 Print out model.summary
model.summary()

epochs = 3

history = model.fit(x_train, 
                    y_train, 
                    epochs=epochs, 
                    batch_size=128,
                    verbose=1,
                    validation_split = 0.2)

scores = model.evaluate(x_test, y_test, verbose=2)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

"""** 6. Stacking LSTM Layers **

In the above model, add another LSTM layer and change where necessary, then train and evaluate the model.
"""

nb_units = 50
seq_length = 28
feature_size = 28
model = Sequential()
model.add(LSTM(nb_units,return_sequences=True, input_shape=(seq_length, feature_size)))
# Adding another LSTM Layer.
model.add(LSTM(nb_units))
model.add(Dense(units=10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

epochs = 3

history = model.fit(x_train, 
                    y_train, 
                    epochs=epochs, 
                    batch_size=128,
                    verbose=1,
                    validation_split = 0.2)

scores = model.evaluate(x_test, y_test, verbose=2)
print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))

